---
import BaseHead from '../components/BaseHead.astro';
import Header from '../components/Header.astro';
import Footer from '../components/Footer.astro';
import { SITE_TITLE, SITE_DESCRIPTION } from '../consts';
---

<!doctype html>
<html lang="en">
	<head>
		<BaseHead title={SITE_TITLE} description={SITE_DESCRIPTION} />
	</head>
	<body><div class="grid-container">
		<Header title={SITE_TITLE} />
		<main>
			<p>
				Hi!
				I'm a machine learning researcher interested in probabilistic methods.
			</p>
			<p>
  			I am currently working on code generation.
  			I've previously dabbled in methods for efficient human-robot interaction.
  			In particular, methods from	Bayesian optimization, experimental design, and program synthesis
  			for optimal and trustworthy communication.
   		</p>
			<p>
        A bit about me: I previously hacked on swe-agents at Cohere.
				I obtained my PhD from Cornell, advised by Sasha Rush.
				I started grad school at Harvard, also with Sasha.
     		Before that, I was a research engineer at Facebook AI Research.
       	And before all that, I scraped by as an undergrad at UPenn CIS.
     	</p>

        	<hr>

        	<h3>Research Topics</h3>
        	<div class="highlights">
            	<b>Software engineering agents</b>
            	<br>
                How can we make robots better software engineers?
                As they become better engineers, do the principles that guide human-oriented design still hold for robots?
            	<ul>
                	<li><a href="https://arxiv.org/abs/2506.11058">Refactoring Codebases through Library Design</a>
                	<li><a href="https://arxiv.org/abs/2412.01769">Commit0: Library Generation from Scratch</a> (ICLR 2025)
                </ul>
        	</div>
        	<div class="highlights">
            	<b>Interaction as optimal control</b>
            	<br>
            	In human-robot interaction, many tasks are too complex to accomplish in a single turn.
            	How can robots collaborate with humans by resolving ambiguity as efficiently as possible?
            	We frame interaction as an optimal control problem, and explore simple heuristics.
            	<ul>
                	<li><a href="https://arxiv.org/abs/2310.17140">Symbolic Planning and Code Generation for Grounded Dialogue</a> (EMNLP 2023)
                	<li><a href="https://arxiv.org/abs/2311.08584">Asking More Informative Questions for Grounded Retrieval</a> (ACL Findings 2024)
                </ul>
        	</div>
        	<div class="highlights">
            	<b>Scaling discrete latent variable models</b>
            	<br>
            	Discrete structure is common in the world (language, biology, code),
            	and can also yield efficient or interpretable models.
            	However, discrete structure makes learning difficult due to non-differentiability.
                Can we scale models with discrete structure?
                And what structural properties can we take advantage of?
                <ul>
                    <li><a href="https://arxiv.org/abs/2406.07524">Simple and Effective Masked Diffusion Language Models</a> (NeurIPS 2024)
                    <li><a href="https://arxiv.org/abs/2503.09573">Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models
</a> (ICLR 2025)
                	<li><a href="https://arxiv.org/abs/2011.04640">Scaling Hidden Markov Language Models</a> (EMNLP 2020)
                	<li><a href="https://proceedings.neurips.cc/paper/2021/hash/16c0d78ef6a76b5c247113a4c9514059-Abstract.html">Low-Rank Constraints for Fast Inference in Structured Models</a> (NeurIPS 2021)
                	<li><a href="https://arxiv.org/abs/2305.14237">HOP, UNION, GENERATE: Unsupervised Multi-hop Reasoning</a> (EMNLP 2023)
                </ul>
        	</div>
		</main>
		<Footer />
	</div></body>
</html>
