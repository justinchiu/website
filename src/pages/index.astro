---
import BaseHead from '../components/BaseHead.astro';
import Header from '../components/Header.astro';
import Footer from '../components/Footer.astro';
import { SITE_TITLE, SITE_DESCRIPTION } from '../consts';
---

<!doctype html>
<html lang="en">
	<head>
		<BaseHead title={SITE_TITLE} description={SITE_DESCRIPTION} />
	</head>
	<body><div class="grid-container">
		<Header title={SITE_TITLE} />
		<main>
			<p>
				Hi!
				I'm a machine learning researcher interested in probabilistic approaches to
				natural language processing.
			</p>
			<p>
    			I'm currently working on methods for efficient human-robot interaction.
    			In particular, I apply methods from
    			Bayesian optimization, experimental design, and program synthesis
    			for optimal and trustworthy communication.
    		</p>
			<p>
				A bit about me: I am a final year PhD student at Cornell, advised by Sasha Rush.
				I started grad school at Harvard, also with Sasha.
        		Before that, I was a research engineer at Facebook AI Research.
            	And before all that, I scraped by as an undergrad at UPenn CIS.
        	</p>

        	<hr>

        	<h3>Research Topics</h3>
        	<div class="highlights">
            	<b>Interaction as optimal control</b>
            	<br>
            	In human-robot interaction, many tasks are too complex to accomplish in a single turn.
            	How can robots successfully collaborate with humans in as few turns as possible?
            	We frame interaction as an optimal control problem, and explore simple heuristics.
            	<ul>
                	<li><a href="https://arxiv.org/abs/2311.08584">Asking More Informative Questions for Grounded Retrieval</a> (preprint)
                	<li><a href="https://arxiv.org/abs/2310.17140">Symbolic Planning and Code Generation for Grounded Dialogue</a> (EMNLP 2023)
                </ul>
        	</div>
        	<div class="highlights">
            	<b>Scaling discrete latent variable models</b>
            	<br>
            	Discrete structure is common in the world (language, biology, code),
            	and can also yield efficient or interpertable models.
            	However, discrete structure is often seen as an obstacle towards efficient learning.
                Can we scale models with discrete structure?
                And what structural properties can we take advantage of?
                <ul>
                	<li><a href="https://arxiv.org/abs/2305.14237">HOP, UNION, GENERATE: Explainable Multi-hop Reasoning without Rationale Supervision</a> (EMNLP 2023)
                	<li><a href="https://proceedings.neurips.cc/paper/2021/hash/16c0d78ef6a76b5c247113a4c9514059-Abstract.html">Low-Rank Constraints for Fast Inference in Structured Models</a> (NeurIPS 2021)
                	<li><a href="https://arxiv.org/abs/2011.04640">Scaling Hidden Markov Language Models</a> (EMNLP 2020)
                </ul>
        	</div>
		</main>
		<Footer />
	</div></body>
</html>
