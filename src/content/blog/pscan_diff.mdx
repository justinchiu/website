---
title: 'Differentiating through an associative parallel scan'
description: 'Blogging about working through some calculus busywork'
pubDate: 'Jan 8 2024'
---

State-space models and linear (matrix-valued) RNNs
have recently risen greatly in popularity,
thanks to efficient hardware implementations.
In particular, parallel associative scans allow one to compute
reductions of sequences of length $$T$$ in $$O(\log T)$$ time
on parallel hardware.

Implementing these parallel reductions requires tremendous care:
naively, they would require much more memory than a sequential $$O(T)$$ scan.
In practice, people get around this via checkpointing and bespoke
implementations of the forward and backward passes.
Manual implementation of backward passes is painful -- this is the whole problem
automatic differentiation is supposed to solve!

In this blog post, we're going to try working through how to automatically
write out the backwards pass of an associative scan.
This post is inspired by a problem my advisor, Sasha,
ran into when implementing state space models.

# Parallel scans
Say we have a sequence of scalars, $$x_1, x_2,\ldots,x_5$$.
Given a sequence of partial product, i.e. the cumulative product,
$$
\begin{equation}
h_t = x_t \oplus h_{t-1} = x_t \oplus x_{t-1} \oplus \cdots \oplus x_1,
\end{equation}
$$
we want to optimize a loss function
$$L(h_1,\ldots,h_5)$$
via gradient descent.

\topherefore, our goal is to efficiently compute the following gradient efficiently:
$$
\begin{align}
\frac{\partial L}{\partial x_i}
&= \sum_{t=i}^T \frac{\partial L}{\partial h_t}\frac{\partial h_t}{\partial x_i}\\
&= \sum_{t=i}^T \frac{\partial L}{\partial h_t}\frac{\partial h_t}{\partial h_{t-1}}
    \cdots\frac{\partial h_i}{\partial x_i}.
\end{align}
$$

# Recurrent form
To make things more amenable to writing in a recurrent form, as in equation (1),
we can compute the transpose of the gradient:
$$
\begin{align}
\frac{\partial L}{\partial x_i}^\top
&= \sum_{t=i}^T 
    \frac{\partial h_i}{\partial x_i}^\top
    \cdots\frac{\partial h_t}{\partial h_{t-1}}^\top
    \frac{\partial L}{\partial h_t}^\top\\
&= \frac{\partial h_i}{\partial x_i}^\top
    \frac{\partial L}{\partial h_i}^\top
    + \frac{\partial h_i}{\partial x_i}
    \frac{\partial h_{i+1}}{\partial h_i}^\top
    \frac{\partial L}{\partial h_{i+1}}^\top + \cdots\\
&= \frac{\partial h_i}{\partial x_i}^\top
    \left(\frac{\partial L}{\partial h_i}^\top
    + \frac{\partial h_{i+1}}{\partial h_i}^\top
    \frac{\partial L}{\partial h_{i+1}}^\top + \cdots\right)\\
&= \frac{\partial h_i}{\partial x_i}^\top
    \left(\frac{\partial L}{\partial h_i}^\top
    + \frac{\partial h_{i+1}}{\partial h_i}^\top
    \left(\frac{\partial L}{\partial h_{i+1}}^\top + \cdots\right)\right).
\end{align}
$$
This can be written as a reverse process as follows:
$$
\begin{equation}
g_i = \frac{\partial L}{\partial h_t}^\top+\frac{\partial h_{i+1}}{\partial h_{i}}^\top g_{i+1},
\end{equation}
$$
and
$$
\begin{equation}
\frac{\partial L}{\partial x_i}^\top = \frac{\partial h_i}{\partial x_i}g_i.
\end{equation}
$$

However, this does not easily admit a parallel scan implementation,
as it is not clear what the associative operator would be.

# Grid form
Let's trek all the way back to equation (3), and write things out in full
for a couple steps:
$$
\frac{\partial L}{\partial x_i}
= \sum_{t=i}^T \frac{\partial L}{\partial h_t}\frac{\partial h_t}{\partial h_{t-1}}
    \cdots\frac{\partial h_i}{\partial x_i}.
$$


$$
\begin{array}{ccccc}
  & \frac{\partial h_i}{\partial x_i}^\top &  & & \frac{\partial L}{\partial h_i}\\
+ & \frac{\partial h_i}{\partial x_i}^\top & \frac{\partial h_{i+1}}{\partial h_i} & & \frac{\partial L}{\partial h_{i+1}}\\
+ & \frac{\partial h_i}{\partial x_i}^\top & \frac{\partial h_{i+1}}{\partial h_i} & \frac{\partial h_{i+2}}{\partial h_{i+1}} & \frac{\partial L}{\partial h_{i+2}}\\
\end{array}
$$
