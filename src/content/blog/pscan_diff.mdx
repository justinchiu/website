---
title: 'Differentiating through an associative parallel scan'
description: 'Blogging about working through some calculus busywork'
pubDate: 'Jan 8 2024'
---

State-space models and linear (matrix-valued) RNNs
have recently risen greatly in popularity,
thanks to efficient hardware implementations.
In particular, parallel associative scans allow one to compute
reductions of sequences of length $$T$$ in $$O(\log T)$$ time
on parallel hardware.

Implementing these parallel reductions requires tremendous care:
naively, they would require much more memory than a sequential $$O(T)$$ scan.
In practice, people get around this via checkpointing and bespoke
implementations of the forward and backward passes.
Manual implementation of backward passes is painful -- this was the whole problem
automatic differentiation is supposed to solve!

In this blog post, we're going to try working through how to automatically
write out the backwards pass of an associative scan.
This post is inspired by a problem my advisor, Sasha,
ran into when implementing state space models.

# Parallel scans
Say we have a sequence of scalars, $$x_1, x_2,\ldots,x_5$$.
Given a sequence of partial product, i.e. the cumulative product,
$$
\begin{equation}
h_t = x_t \oplus h_{t-1} = x_t \oplus x_{t-1} \oplus \cdots \oplus x_1,
\end{equation}
$$
we want to optimize a loss function
$$L(h_1,\ldots,h_5)$$
via gradient descent.

\topherefore, our goal is to efficiently compute the following gradient efficiently:
$$
\begin{align}
\frac{\partial L}{\partial x_i}
&= \sum_{t=i}^T \frac{\partial L}{\partial h_t}\frac{\partial h_t}{\partial x_i}\\
&= \sum_{t=i}^T \frac{\partial L}{\partial h_t}\frac{\partial h_t}{\partial h_{t-1}}
    \cdots\frac{\partial h_i}{\partial x_i}.
\end{align}
$$

# Recurrent form
To make things more amenable to writing in a recurrent form, as in equation (1),
we compute the transpose of the gradient:
$$
\begin{align}
\frac{\partial L}{\partial x_i}^\top
&= \sum_{t=i}^T 
    \frac{\partial h_i}{\partial x_i}^\top
    \cdots\frac{\partial h_t}{\partial h_{t-1}}^\top
    \frac{\partial L}{\partial h_t}^\top\\
&= \frac{\partial h_i}{\partial x_i}^\top
    \frac{\partial L}{\partial h_i}^\top
    + \frac{\partial h_i}{\partial x_i}
    \frac{\partial h_{i+1}}{\partial h_i}^\top
    \frac{\partial L}{\partial h_{i+1}}^\top + \cdots\\
&= \frac{\partial h_i}{\partial x_i}^\top
    \left(\frac{\partial L}{\partial h_i}^\top
    + \frac{\partial h_{i+1}}{\partial h_i}^\top
    \frac{\partial L}{\partial h_{i+1}}^\top + \cdots\right)\\
&= \frac{\partial h_i}{\partial x_i}^\top
    \left(\frac{\partial L}{\partial h_i}^\top
    + \frac{\partial h_{i+1}}{\partial h_i}^\top
    \left(\frac{\partial L}{\partial h_{i+1}}^\top + \cdots\right)\right).
\end{align}
$$
This can be written as an input-dependent associative scan as follows:
$$
\begin{equation}
g_t = \frac{\partial h_i}{\partial }\frac{\partial L}{\partial h_t}
\end{equation}
$$

# Grid form
